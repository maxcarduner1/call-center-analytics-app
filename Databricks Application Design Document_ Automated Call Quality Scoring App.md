# Databricks Application Design Document: Automated Call Quality Scoring App

# 1\. Introduction

This document details the design and architecture for a simple, data-driven web application hosted on Databricks. The applicationâ€™s primary function is to display call center transcripts and their corresponding automated quality scorecards generated by an AI model. The system utilizes Databricks for data processing and scoring, with a **Reverse Synchronization Pipeline** pushing the final, clean data to an autoscaling PostgreSQL backend (**Lakebase PostgreSQL**) for the application's serving layer.

# 2\. Goals and Objectives

1. **Automated Scoring:** Implement a reliable pipeline to automatically score call transcripts using an LLM based on defined criteria.  
2. **Data Centralization:** Maintain the definitive source of truth (Lakebase) within Delta Lake on Databricks.  
3. **Application Serving:** Provide a dedicated, high-performance, autoscaling PostgreSQL database for the external simple web application to consume data.  
4. **Reverse Synchronization:** Ensure near real-time data consistency between the Databricks Delta Lake table and the PostgreSQL serving table.

# 3\. Data Model

## 3.1 Core Entity: Call Record Table

The primary data entity is the `call_center_scores` table, stored both in Delta Lake (Gold Layer) and replicated to the PostgreSQL backend.

| Column Name | Data Type (Delta Lake) | Data Type (PostgreSQL) | Constraints | Description |
| :---- | :---- | :---- | :---- | :---- |
| `call_id` | `TEXT` | `VARCHAR(255)` | PRIMARY KEY | Unique identifier for the call. |
| `member_id` | `TEXT` | `VARCHAR(255)` |  | Identifier for the member. |
| `call_date` | `TEXT` (Prefer `TIMESTAMP` in production) | `TIMESTAMP` |  | Date and time the call occurred. |
| `transcript` | `TEXT` | `TEXT` |  | The full text transcript of the call. |
| `scorecard` | `STRUCT` (as defined below) | `JSONB` |  | LLM-generated quality scorecard in JSON structure. |

## 3.2 Scorecard Schema (`scorecard` column)

The `scorecard` column stores the structured output from the LLM, enabling easy querying and visualization.

### LLM Output Structure (JSON and Databricks SQL `STRUCT`)

**Prompt Used:** `COC_Scorecard_Prompt`

**Expected Return Type (`returnType`):**

```
STRUCT<
 criteria_1: STRUCT<
   technical_aspects: STRUCT<
     recording_disclosure: STRUCT<score: INT>,
     member_authentication: STRUCT<score: INT>,
     call_closing: STRUCT<score: INT>
   >
 >,
 criteria_2: STRUCT<
   quality_of_service: STRUCT<
     professionalism: STRUCT<score: INT>,
     program_information: STRUCT<score: INT>,
     demeanor: STRUCT<score: INT>
   >
 >,
 total_score: INT
>
```

The `total_score` is derived by summing the scores from all underlying criteria.

# 4\. Architecture and Data Flow

## 4.1 Data Pipeline Overview

The architecture follows a Medallion structure (Bronze \-\> Silver \-\> Gold), with the addition of the reverse synchronization step to the external serving layer.

### Phase 1: Ingestion and Cleaning (Bronze/Silver)

1. **Raw Ingestion:** Raw call transcripts and metadata land in cloud storage.  
2. **Delta Lake:** Data is loaded into a Bronze Delta table.  
3. **Preparation:** A Databricks Delta Live Table (DLT) pipeline or Structured Streaming job cleanses and enriches the data into a Silver table, ready for scoring.

### Phase 2: Automated Scoring (Gold)

1. **Scoring Job:** A Databricks Workflow (scheduled job) runs a Spark process on the Silver table.  
2. **LLM Call:** For each new `transcript`, a User Defined Function (UDF) or direct API call interacts with an LLM (e.g., Databricks Foundation Model API or external service like OpenAI).  
   * **Input:** `transcript` \+ `COC_Scorecard_Prompt`.  
   * **Output:** Validated JSON structure.  
3. **Data Persistence:** The generated `scorecard` is added to the record, which is written to the **Gold Delta Table (`call_center_scores`)**.

### Phase 3: Reverse Synchronization (Serving Layer)

This is the critical step that bridges the Lakehouse (Databricks) with the operational application database (PostgreSQL).

1. **Synchronization Mechanism:** A continuous DLT pipeline or a highly available Structured Streaming job is configured.  
2. **Source:** Gold Delta Table (`call_center_scores`).  
3. **Sink (Reverse Sync):** The autoscaling PostgreSQL database.  
4. **Operation:** The pipeline monitors the Gold table for new rows or updates to existing `scorecard` entries. It uses a **JDBC Sink** to perform `UPSERT` operations into the PostgreSQL table (`app_calls`), keyed on `call_id`. This ensures the PostgreSQL database always reflects the latest state of the data managed in Databricks.

## 4.2 Application Serving Layer

| Element | Technology/Component | Function |
| :---- | :---- | :---- |
| **Backend DB** | Lakebase Autoscaling DB | High-availability, low-latency data source for the web application. Handles the app's read and any manual update requests. |
| **Simple Web App** | Databricks App HTML/CSS with python fast api server | User interface for viewing calls, transcripts, and the detailed LLM scorecards. |

# 5\. Security and Governance

## 5.1 Data Access Control

* **Databricks:** Unity Catalog will enforce granular access control on the Bronze, Silver, and Gold Delta tables. Only the Scoring and Sync pipelines will have write access to the Gold table.  
* **PostgreSQL:** Dedicated, minimal-privilege user credentials will be created for the web application (Read-Only access is ideal, or Read/Write if manual overrides are allowed).

